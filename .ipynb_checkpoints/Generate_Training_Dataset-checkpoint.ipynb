{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generator notebook for Magnetic Signal reconstruction\n",
    "\n",
    "This notebook describes explicitly the magnetic field component dataset generator's first version. Because Matlab requests specifics version of python to load its engine, we are using an old python version. Python 2.7 seems to be more stable with the Matlab version I had (R2016b) regarding long-running time and potential data leaks. The versions can be updated easily. I wrote the code allowing it to run on a Python3 environment needing as fewer modifications as possible.\n",
    "\n",
    "I don't recommand to use this Notebook to generate the training set, using a python script is much easier and faster.\n",
    "To generate a .py script from a notebook (.ipynb) run this command:\n",
    "\n",
    "`jupyter nbconvert --to python notebook_name.ipynb`\n",
    "\n",
    "This will generate a python script (with all the comments included) you can run in background using for example:\n",
    "\n",
    "`( nohup python script.py & )` \n",
    "\n",
    "`nohup` launches the script as a background job. If you launch this through `ssh` the job will be killed after you close the session. To avoid that, add parenthesis to run it in a subshell. I recommand using `nohup` to keep trace of what's happening through execution in the nohup file. It can be consulted any time of the execution using `tail -f nohup.out` to access in real time the last written bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui tk \n",
    "#Remove in the .py file\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning) \n",
    "import matlab.engine\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "from scipy.io import loadmat\n",
    "from collections import defaultdict\n",
    "from os import system\n",
    "import math\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, normalize\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import clear_output\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import tkinter.font as TkFont\n",
    "import threading\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python allows us to run Matlab functions in background and retrieve their output. The Output format will, of course, be specific, for example, a `float ` output coming out from a Matlab function will be interpreted as `matlab.double` for Linux and `matlab.mlarray.double` for Windows.\n",
    "All of these can be reformatted to native python variables. We now can call Matlab functions from the `eng` object. \n",
    "\n",
    "__*Only functions referenced in the installed Matlab's path can be called from the engine*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = matlab.engine.start_matlab() #starting and storing the matlab engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables instantiation\n",
    "The below cell instantiates all the variables we will use in the execution. Working with this amount of `public` variables is not a good practice. But for development purpose and easier debugging time (Jupyter doesn't have any debugger natively), instantiating them as `public` allows us to access them at any time in the process easily. One improvement would be to install an external Debugging plugin to Jupyter-lab and transpose all the `public` variables to `private`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quietDays = np.array(eng.quiet()) # we retreive the output of the `quiet()` matlab function, convert it as `numpy.array` and finally store it.\n",
    "\n",
    "# Setting up the paths (relative path obviously, needs to be changed if the folder structure changes)\n",
    "trainingDatasetPath = \"../TrainingDataset\"\n",
    "trainingDatasetPathASIA = \"../TrainingDataset/Asia/\"\n",
    "\n",
    "# load the stations list\n",
    "mat = loadmat(\"../../../../opt/dir.indices/station.mat\")\n",
    "\n",
    "# store the \"station\" column values from the .mat file \n",
    "stationsList = mat.get(\"station\", \"none\")[0]\n",
    "stationCodesID = dict()\n",
    "stationIDlatMag = dict()\n",
    "allStationCodes=np.array([])\n",
    "allStationLatgeos=np.array([])\n",
    "allStationLongeos=np.array([])\n",
    "\n",
    "# store in separate arrays all the stations code (clf, aae...) and their geographic latitudes\n",
    "a=1\n",
    "for x in stationsList:\n",
    "    stationIDlatMag[x[1][0]] = int(x[10][0][0])\n",
    "    stationCodesID[x[1][0]] = a\n",
    "    allStationCodes=np.append(allStationCodes,x[1][0])\n",
    "    allStationLatgeos=np.append(allStationLatgeos,x[3][0])\n",
    "    allStationLongeos = np.append(allStationLongeos,x[2][0])\n",
    "    a+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChoosePresetArea(`str`, `list`)\n",
    "\n",
    "Outputs a list of max/min longitudes and latitudes according to specific preset, mainly for a gain of time. For now 3 presets are available and can be selected by the name of the area. For example, 'america' will return [30,54,240,307] corresponding to min lat, max lat, min long, max long. There is obviously a way to enter a custom area by selecting 'custom' and writing the degrees of the wanted zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChoosePresetArea(area,customArea):\n",
    "    america = [30,54,240,307]\n",
    "    asia = [30,54,57,159]\n",
    "    europe = [30,54,0,50]\n",
    "    laMin = 0\n",
    "    laMax = 0 \n",
    "    lonMin = 0 \n",
    "    lonMax = 0\n",
    "    if area == 'america':\n",
    "        laMin=america[0]\n",
    "        laMax=america[1]\n",
    "        lonMin=america[2]\n",
    "        lonMax=america[3]\n",
    "    elif area == 'asia':\n",
    "        laMin=asia[0]\n",
    "        laMax=asia[1]\n",
    "        lonMin=asia[2]\n",
    "        lonMax=asia[3]\n",
    "    elif area == 'europe':\n",
    "        laMin=europe[0]\n",
    "        laMax=europe[1]\n",
    "        lonMin=europe[2]\n",
    "        lonMax=europe[3]\n",
    "    elif area == 'custom':\n",
    "        laMin=customArea[0]\n",
    "        laMax=customArea[1]\n",
    "        lonMin=customArea[2]\n",
    "        lonMax=customArea[3]\n",
    "    return laMin, laMax, lonMin, lonMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenerateTrainingSet(`str`, `list`, `list`, `int`, `int`, `int`, `int`, `int`, `int`, `str`)\n",
    "\"main\" function which launches all the others and save the generated arrays as .npy files. (.npy files store arrays in binary and are readable by `numpy`, processing them is way faster than the usual `panda` datasets and the storing space is significantly shorten.\n",
    "\n",
    "The `def` starts a loop that lasts equally to the `dataSetSize` we want. It increments at each loop one day to the absolute starting date, requests Matlab on the new time bound, transforsms the output in arrays, normalizes them and finally stores everyting.\n",
    "\n",
    "#### Parameters\n",
    "There are 10 initial parameters allowing us to have full control over the generation. Which indice do we want? From which starting date to which ending date? Which are the min/max latitudes and min/max longitudes boundaries? How many days per matrix do we want? How many minutes are needed between each values? And finally, what is the machine learning algorithm we are using for the reconstruction.\n",
    "\n",
    "The Machine Learning algorithm parameter has 3 possibilities: \n",
    "\n",
    "* `svr` : for the Support Vector Machine\n",
    "- `pr` : for the Polynomial Regression\n",
    "- `rfr` : for the Random Forest Regression\n",
    "\n",
    "\n",
    "#### Storing format\n",
    "We store three arrays. The two first of shape (24,144), 24 degrees in latitude for 144 values each. The third array of shape (12, ) storing all the informations we need for the current day. All arrays gathered we get an array of shape (3, ) symbolising an array of three arrays.\n",
    "\n",
    "|Ground truth|Machine learning reconstruction|Infos|\n",
    "|:-:|:-:|:-:|\n",
    "|Absolute ground truth got from Matlab without any modification|The Ground truth with all nan values filled with ML|Informations about the current matrix|\n",
    "|(24,144)|(24,144)|(12, )|\n",
    "\n",
    "The information array is constituted of:\n",
    "\n",
    "|Date|Max latitude|Min latitude|Max longitude|Min longitude|Max value|Min value|Days|isQuiet|Component type|Working stations|Area|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|Date of the current matrix stored as datetime object|Latitude degree corresponding to the last index of the array|Latitude degree corresponding to the first index of the array|The maximum longitude degree in which our stations are located|The minimum longitude degree in which are located our stations|Maximum value in the matrix (in nT)|Minimum value in the matrix (in nT)|Number of days in the matrix|Is the current matrix conrresponding to a quiet day?|The component type (x1, x2, y1, y2, alpha etc)|List all working stations name for the given matrix|In which Area are we? (EU, USA, Asia or Custom)|\n",
    "|datetime.datetime|Integer|Integer|Integer|Integer|Float|Float|Integer|Bool|String|List|String|\n",
    "\n",
    "Note that we have to load these arrays using:\n",
    "```python\n",
    "array = np.load(\"path_to_array/array.npy\", allow_pickle=True, encoding=\"latin1\")\n",
    "```\n",
    "`allow_pickle=True` and `encoding=\"latin1\"` allows us to load `object` values into `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTrainingSet(indice, \n",
    "                        startingDate, \n",
    "                        endingDate, \n",
    "                        area,\n",
    "                        numberOfDaysPerMatrix, \n",
    "                        minutesBetweenValues, \n",
    "                        regressor,\n",
    "                        customArea=[0,0,0,0],\n",
    "                        interface=False,\n",
    "                        application=None):\n",
    "\n",
    "    absoluteStartDate = datetime.datetime(startingDate[0],startingDate[1],startingDate[2],0,0,0)\n",
    "    absoluteEndDate = datetime.datetime(endingDate[0],endingDate[1],endingDate[2],0,0,0)\n",
    "    isQuietDay = False\n",
    "    dataSetSize = absoluteEndDate-absoluteStartDate\n",
    "    \n",
    "    for i in range(0,dataSetSize.days):\n",
    "        latMin, latMax, longMin, longMax = ChoosePresetArea(area,customArea)\n",
    "        \n",
    "        startDateMatrix = absoluteStartDate+datetime.timedelta(days=i)\n",
    "        startDateMatlab = [startDateMatrix.year, startDateMatrix.month, startDateMatrix.day, 0, 0, 0]\n",
    "        sys.stdout.flush() # flushes the verbose output allowing us to read everything on time if we launch the script in a nohup subshell\n",
    "        \n",
    "        groundTruth = RequestMatlabNew(allStationCodes, allStationLatgeos, allStationLongeos, latMin, latMax, longMin, longMax, startDateMatlab)\n",
    "        groundTruthNorm = np.empty_like(groundTruth)\n",
    "        for ilat in range(groundTruth.shape[0]):\n",
    "            groundTruthNorm[ilat] = normalizeWithGivenBounds(groundTruth[ilat],{'actual': {'lower': np.nanmin(groundTruth), 'upper': np.nanmax(groundTruth)}, 'desired': {'lower': -1, 'upper': 1}})\n",
    "        \n",
    "\n",
    "        beforeVariance = groundTruthNorm.copy()\n",
    "        groundTruthNorm = RemoveDefectiveStationVariance(beforeVariance)\n",
    "\n",
    "        ReconstructedArray = PredictIndicatorForAllLatitudes(groundTruthNorm, regressor)\n",
    "        for a in quietDays:\n",
    "            compare = np.array(startDateMatlab) == a\n",
    "            if compare.all(): \n",
    "                isQuietDay = True\n",
    "                break\n",
    "            else: \n",
    "                isQuietDay = False \n",
    "                \n",
    "\n",
    "        infosArray = np.array([startDateMatrix, latMax, latMin, longMax, longMin, np.nanmax(groundTruth), np.nanmin(groundTruth), numberOfDaysPerMatrix, isQuietDay, indice, ['n/a'], area])\n",
    "        FinalArray = np.array([groundTruth, ReconstructedArray, infosArray])\n",
    "        np.save(\"{}/x_train/{}_{}\".format(trainingDatasetPathASIA, indice, i), FinalArray)\n",
    "        print(\"Matrix saved for date: {}\".format(startDateMatrix))\n",
    "        print(\"Sample {} out of {}\".format(i+1, dataSetSize.days))\n",
    "        \n",
    "        if interface:\n",
    "            totalSamples=float(dataSetSize.days)\n",
    "            sampleNum=float(i+1)\n",
    "\n",
    "            UpdateInterface(totalSamples, sampleNum, application, str(\"Matrix saved for date: {}\".format(startDateMatrix)), str(\"Sample {} out of {}\".format(int(sampleNum), int(totalSamples))), int((sampleNum/totalSamples)*100))\n",
    "            application.update_idletasks()\n",
    "        clear_output(wait=True)\n",
    "        del groundTruth, beforeVariance, ReconstructedArray, infosArray, FinalArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RemoveDefectiveStation(`numpy.array`)\n",
    "Takes as input the matrix we are working with and replace by nans all the axis with a `mean_squared_error` too far from the `mean_squared_error` of the mean of all the axis.\n",
    "\n",
    "This `def` allows us to considere as non-existant all the stations outputing delusional data. For example, the green line in the middle of the matrix below represents a station outputing zeros for the entire day straight, this `def` will alow us to take it off.\n",
    "\n",
    "\n",
    "<img src=\"Notebook_images/DelusionalDataGreen.jpg\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveDefectiveStationVariance(array):\n",
    "    for i in range(array.shape[0]):\n",
    "        if np.var(array[i]) < 0.1: #valeur arbitraire\n",
    "            array[i] = np.full(array.shape[1], np.nan)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RequestMatlab()\n",
    "Stores all the `indices_alpha` output in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RequestMatlabNew(allStationCodes, allStationLatgeos, allStationLongeos, latMin, latMax, longMin, longMax, startDateMatlab):\n",
    "    stInMat = []\n",
    "    for i in range(0,allStationCodes.shape[0]): \n",
    "        if allStationLatgeos[i]>latMin and allStationLatgeos[i]<latMax and allStationLongeos[i]>longMin and allStationLongeos[i]<longMax:\n",
    "            stInMat.append(allStationCodes[i])\n",
    "    stID = []\n",
    "    stID_latMag = []\n",
    "    for stCode in stInMat:\n",
    "        stID.append(stationCodesID.get(stCode))\n",
    "    err = StringIO()\n",
    "    out = StringIO()\n",
    "    mat,T,LAT = eng.create_matrix(matlab.double(stID),matlab.double(startDateMatlab),'y2','m', latMin, latMax-1, 1./5.99, 1, nargout=3, stderr=err, stdout=out)\n",
    "    groundTruthMag = np.array(mat)\n",
    "    return groundTruthMag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalizeWithGivenBounds(`numpy.array`, `numpy.array`)\n",
    "Normalizes all the values of a vector between the wanted bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeWithGivenBounds(values, bounds):\n",
    "    return [bounds['desired']['lower'] + (x - bounds['actual']['lower']) * (bounds['desired']['upper'] - bounds['desired']['lower']) / (bounds['actual']['upper'] - bounds['actual']['lower']) for x in values]               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PredictIndicatorForAllLatitudesdes(`numpy.array`)\n",
    "\n",
    "Main Machine Learning function that triggers the polynomial regression for all longitude degrees.\n",
    "\n",
    "We want to train deep learning algorithms on full matrices to test their ability to reconstruct data in controled situations. Therefore, we will be able reproduce the behaviour of the magnetic field in any contextual circumstances. Consequently, the objective here is to do a preliminary reconstruction on highly covered areas like Europe to feed the deep learning algorithms with matrices without any `nan`.\n",
    "\n",
    "The `def` takes all the points from all working stations between two latitude bounds, removes the `nan` values for latitudes we don't have data and fits a polynomial regression on the remaining points. The result will be a matrix with the same dimension as the previous one but with all the blank lines filled.\n",
    "\n",
    "For example, let's say that we want to fill all `nan` values of the matrix below matrix : \n",
    "\n",
    "<img src=\"Notebook_images/Before_reconstruction.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "We would have to take this matrix values for every position on the X axis, let's take as example at the red line position :\n",
    "\n",
    "<img src=\"Notebook_images/Before_reconstruction_redLine.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "This \"cut\" would give us the component's values in regard to the latitude, a 2D representation easy to fit :\n",
    "\n",
    "<img src=\"Notebook_images/ML_processing_0.png\" alt=\"drawing\" width=\"400\"/> And after all the blanks filled : <img src=\"Notebook_images/ML_processing_23.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Doing this for all latitude values, this process will output this result:\n",
    "\n",
    "<img src=\"Notebook_images/After_reconstruction.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictIndicatorForAllLatitudes(baseArray, regressor):\n",
    "    predictionArray = np.empty_like(baseArray)\n",
    "    predictionArray=np.copy(baseArray)\n",
    "    RegressorParameters=None\n",
    "    RegressorParametersPR = {'polynomialfeatures__degree': 2, 'linearregression__fit_intercept': True, 'linearregression__normalize': True} # For PolyRegression\n",
    "    RegressorParametersSVR = {'kernel' : 'rbf', 'gamma' : 1e-2, 'C' : 10} # For SupportVectorMachinregressorression\n",
    "    RegressorParametersRFR = {'n_estimators' : 10, 'random_state' : 0} # For RandomForestRegression\n",
    "    if regressor=='svr': RegressorParameters = RegressorParametersSVR\n",
    "    elif regressor=='pr': RegressorParameters = RegressorParametersPR\n",
    "    elif regressor=='rfr': RegressorParameters = RegressorParametersRFR\n",
    "        \n",
    "    for i in range(0,baseArray.shape[0]): # for all degrees in latitude\n",
    "        specificLatitudeTimePrediction = np.full(baseArray.shape[1], np.nan)\n",
    "        if math.isnan(np.sum(baseArray[i])): # is there any nan in the selected latitude ?\n",
    "            for y in range(0,baseArray.shape[1]): # for all degrees in longitude\n",
    "                if (math.isnan(baseArray[i][y])):  # if the current point in the matrix is a nan\n",
    "                    specificLatitudeTimePrediction[y] = GetIndicatorLongPrediction(i,y,RegressorParameters,predictionArray,regressor) # start the Machine Learning algorithm  \n",
    "            predictionArray[i] = specificLatitudeTimePrediction # add the predicted values to the global prediction\n",
    "    return predictionArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GetIndicatorLongPrediction(`int`,`int`, `dict`, `numpy.array`)\n",
    "\n",
    "Gets the point of all stations at a given moment in time, concatenates them and removes the nan values. (ML algorithm don't allow nan values).\n",
    "This `def` takes as input a dictionary of parameters, which are hard coded at line 5 of `PredictIndicatorForAllLatitudes`.\n",
    "Before, I was doing parameters tuning at each training to fit the data as best as possible. But it appeared through tests that most of the time the parameters resulting from the process were always the same:\n",
    "```python\n",
    "{'polynomialfeatures__degree': 2, 'linearregression__fit_intercept': True, 'linearregression__normalize': True}\n",
    "```\n",
    "To gain processing time, this functionality is disabled by default. It can be enabled by replacing `RegressorParameters` allocation with the `ParametersTuningPoly(numpy.array,int)` function, which outputs a dictionary of parameters resulted from the tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetIndicatorLongPrediction(latitude,longitude, params, baseArray, regressor):\n",
    "    indicatorLatVariation = np.array([])\n",
    "    prediction=None\n",
    "    for i in range(0, baseArray.shape[0]):\n",
    "        indicatorLatVariation = np.append(indicatorLatVariation, baseArray[i][longitude])\n",
    "    y = np.array(indicatorLatVariation)\n",
    "    x = np.arange(0, baseArray.shape[0], 1)\n",
    "    x,y = RemoveNan(x, y)\n",
    "    if regressor=='svr': prediction = SupportVectorMachinregressorression(x,y,params, latitude, longitude).predict(np.array(latitude).reshape(1,-1))\n",
    "    elif regressor=='pr': prediction = PolyRegression(x,y,params).predict(np.array(latitude).reshape(1,-1))\n",
    "    elif regressor=='rfr': prediction = RandomForestRegression(x,y,params).predict(np.array(latitude).reshape(1,-1))\n",
    "       \n",
    "    return prediction # return the result of the PolyRegression def, defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RemoveNan(`numpy.array`, `numpy.array`)\n",
    "\n",
    "Takes as input all the indice values corresponding to each latitudes of the matrix, detects where there are nans and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNan(latValues, indicatorValues):\n",
    "    indexDeleteY = np.array([])\n",
    "    for i in range(0, indicatorValues.shape[0]):\n",
    "        if math.isinf(indicatorValues[i]) or math.isnan(indicatorValues[i]):\n",
    "            indexDeleteY = np.append(indexDeleteY, i)\n",
    "    newY = np.delete(indicatorValues, indexDeleteY)\n",
    "    newX = np.delete(latValues, indexDeleteY)\n",
    "    newY=newY.reshape(newY.shape[0],1)\n",
    "    newX=newX.reshape(newY.shape[0],1)\n",
    "    \n",
    "    return newX, newY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolyRegression(`numpy.array`, `numpy.array`, `Dict`)\n",
    "\n",
    "Makes a polynomial regression. `poly_grid.fit(X,Y)` where `X` is the latitude and `Y` is the indice. `params` corresponds to the regressor's parameters. \n",
    "The `PolynomialRegression()` definition is custom and detailed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolyRegression(latValues, indicatorValues, params):\n",
    "    poly_grid = PolynomialRegression()\n",
    "    poly_grid.set_params(**params)\n",
    "    poly_grid.fit(latValues, indicatorValues)\n",
    "    return poly_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegression(`numpy.array`, `numpy.array`, `Dict`)\n",
    "\n",
    "Uses the Sklearn random forest regressor. `RandomForestRegressor.fit(X,Y)` where `X` is the latitude and `Y` is the indice. `params` corresponds to the regressor's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestRegression(latValues, indicatorValues, params):\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.set_params(**params)\n",
    "    rf.fit(latValues, indicatorValues)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SupportVectorMachinregressorression(`numpy.array`, `numpy.array`, `Dict`)\n",
    "\n",
    "Uses the Sklearn SVR regressor. `SVR.fit(X,Y)` where `X` is the latitude and `Y` is the indice. `params` corresponds to the regressor's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupportVectorMachinregressorression(latValues, indicatorValues, params, lat, long):\n",
    "    svr = SVR()\n",
    "    svr.set_params(**params)\n",
    "    svr.fit(latValues, indicatorValues)\n",
    "\n",
    "    #     The bellow commented code shows two plots as an example of what ML does for each longitude degrees to reconstruct the full matrix\n",
    "\n",
    "#     if long==38:\n",
    "#         if lat==0 or lat==23:\n",
    "#             fig, ax = plt.subplots(1,1)\n",
    "#             ax.scatter(latValues, indicatorValues, label='Stations values')\n",
    "#             x = np.arange(24)\n",
    "#             pred = svr.predict(x.reshape(-1,1)) \n",
    "#             ax.plot(x.reshape(-1,1),pred, 'r--', label='Model fit')\n",
    "#             ax.xaxis.set_ticks(range(0,24,4))\n",
    "#             ax.xaxis.set_ticklabels(range(30,54,4))\n",
    "#             ax.legend(loc='best')\n",
    "#             ax.set_xlabel('Latitude')\n",
    "#             ax.set_ylabel('Component value')\n",
    "#             fig.savefig('ML_processing_{}.png'.format(lat))\n",
    "    return svr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PolynomialRegression(int, **)\n",
    "\n",
    "Makes a python pipeline out of `sklearn.preprocessing.PolynomialFeatures` and `sklearn.linear_model.LinearRegression`. This allows us to use a linear regression algorithm on a non-linear fit, giving as parameter the polynom's degree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParametersTuningPoly(`numpy.array`,`int`)\n",
    "\n",
    "Makes a quick fit on a given array of data to evaluate the best parameters on the current set. We are testing polynomial degrees from 2 to 5 and check if we should use `linearregression__fit_intercept` or `linearregression__normalize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParametersTuningPoly(baseArray,long):\n",
    "    indicatorLatVariation = np.array([])\n",
    "    for i in range(0, baseArray.shape[0]):\n",
    "        indicatorLatVariation = np.append(indicatorLatVariation, baseArray[i][np.int16(long)])\n",
    "    y = np.array(indicatorLatVariation)\n",
    "    x = np.arange(0, baseArray.shape[0], 1)\n",
    "    x, y = RemoveNan(x, y)\n",
    "    \n",
    "    paramsTuning = {'polynomialfeatures__degree': [2,5], 'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False]}\n",
    "    poly_gridTuning = GridSearchCV(PolynomialRegression(), paramsTuning, cv=10, scoring='r2', verbose=0)\n",
    "    poly_gridTuning.fit(x, y)\n",
    "    return poly_gridTuning.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface(`object`)\n",
    "\n",
    "Create an graphical interface to make the use of the generator more user friendly. It can be triggered by launching the .py script with the `-g` parameter. Typically, the command `python Generate_Training_Dataset.py -g` will start the graphical interface while `python Generate_Training_Dataset.py` won't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interface(tk.Tk):\n",
    "    \n",
    "    def __init__(self):\n",
    "        tk.Tk.__init__(self)\n",
    "        self.theme=ttk.Style()\n",
    "        self.theme.theme_use('clam')\n",
    "        self.CreateWidgets()\n",
    "    \n",
    "    def CreateWidgets(self):\n",
    "        global app_bg_color\n",
    "        self.font = Font(family=\"Comic\", size=12)\n",
    "        self.stickyLabels = 'w'\n",
    "        self.secondBG = \"#ab9c74\"\n",
    "        self.text = tk.Text(self)\n",
    "        self.text.configure(font=self.font)\n",
    "        \n",
    "        \n",
    "        self.mainFrame = tk.Frame(self, bg=app_bg_color)\n",
    "        self.mainFrame.pack()\n",
    "        \n",
    "        self.parametersFrame = tk.Frame(self.mainFrame, bg=self.secondBG)\n",
    "        self.parametersFrame.grid(row=0,column=1,pady=20)\n",
    "        \n",
    "        self.activeWidgetsFrame = tk.Frame(self.mainFrame, bg=self.secondBG)\n",
    "        self.activeWidgetsFrame.grid(row=1,column=1)\n",
    "        \n",
    "        \n",
    "        self.component=tk.Label(self.parametersFrame,text=\"Component\", bg=self.secondBG)\n",
    "        self.component.configure(font=self.font)\n",
    "        self.component.grid(row=1,column=0, sticky=self.stickyLabels)\n",
    "        self.component=tk.Entry(self.parametersFrame)\n",
    "        self.component.insert(tk.END, 'y2')\n",
    "        self.component.grid(row=1,column=1)\n",
    "\n",
    "        self.startDate=tk.Label(self.parametersFrame,text=\"Starting date\", bg=self.secondBG)\n",
    "        self.startDate.configure(font=self.font)\n",
    "        self.startDate.grid(row=2,column=0, sticky=self.stickyLabels)\n",
    "        self.startDate=tk.Entry(self.parametersFrame)\n",
    "        self.startDate.insert(tk.END, '2015,1,1')\n",
    "        self.startDate.grid(row=2,column=1)\n",
    "\n",
    "        self.endDate=tk.Label(self.parametersFrame,text=\"Ending date\", bg=self.secondBG)\n",
    "        self.endDate.configure(font=self.font)\n",
    "        self.endDate.grid(row=3,column=0, sticky=self.stickyLabels)\n",
    "        self.endDate=tk.Entry(self.parametersFrame)\n",
    "        self.endDate.insert(tk.END, '2015,1,6')\n",
    "        self.endDate.grid(row=3,column=1)\n",
    "\n",
    "        self.area=tk.Label(self.parametersFrame,text=\"Area\", bg=self.secondBG)\n",
    "        self.area.configure(font=self.font)\n",
    "        self.area.grid(row=4,column=0, sticky=self.stickyLabels)\n",
    "        self.area=tk.Entry(self.parametersFrame)\n",
    "        self.area.insert(tk.END, 'asia')\n",
    "        self.area.grid(row=4,column=1)\n",
    "\n",
    "        self.days=tk.Label(self.parametersFrame,text=\"Days in the matrix\", bg=self.secondBG)\n",
    "        self.days.configure(font=self.font)\n",
    "        self.days.grid(row=5,column=0, sticky=self.stickyLabels)\n",
    "        self.days=tk.Entry(self.parametersFrame)\n",
    "        self.days.insert(tk.END, '1')\n",
    "        self.days.grid(row=5,column=1)\n",
    "\n",
    "        self.minutes=tk.Label(self.parametersFrame,text=\"Minutes between each value\", bg=self.secondBG)\n",
    "        self.minutes.configure(font=self.font)\n",
    "        self.minutes.grid(row=6,column=0, sticky=self.stickyLabels)\n",
    "        self.minutes=tk.Entry(self.parametersFrame)\n",
    "        self.minutes.insert(tk.END, '10')\n",
    "        self.minutes.grid(row=6,column=1)\n",
    "\n",
    "        self.regressor=tk.Label(self.parametersFrame,text=\"Machine Learning Regressor\", bg=self.secondBG)\n",
    "        self.regressor.configure(font=self.font)\n",
    "        self.regressor.grid(row=7,column=0, sticky=self.stickyLabels)\n",
    "        self.regressor=tk.Entry(self.parametersFrame)\n",
    "        self.regressor.insert(tk.END, 'svr')\n",
    "        self.regressor.grid(row=7,column=1)\n",
    "        \n",
    "        self.launch = tk.Button(self.activeWidgetsFrame, text='Start generator', command= lambda *args : start_GenerateTrainingSet_thread({'indice':self.component.get(), \n",
    "                                                                                                                                        'startingDate':np.fromstring(self.startDate.get(), dtype=int, sep=','), \n",
    "                                                                                                                                        'endingDate':np.fromstring(self.endDate.get(), dtype=int, sep=','), \n",
    "                                                                                                                                        'area':self.area.get(), \n",
    "                                                                                                                                        'numberOfDaysPerMatrix':int(self.days.get()), \n",
    "                                                                                                                                        'minutesBetweenValues':int(self.minutes.get()), \n",
    "                                                                                                                                        'regressor':self.regressor.get(), \n",
    "                                                                                                                                        'customArea':[0,0,0,0],\n",
    "                                                                                                                                        'interface':True,\n",
    "                                                                                                                                        'application':self}))\n",
    "        self.launch.grid(row=1, column=0,pady=5)\n",
    "        \n",
    "        self.kill = tk.Button(self.activeWidgetsFrame, text='Stop generator', command=self.destroy)\n",
    "        self.kill.grid(row=1, column=1,pady=5)\n",
    "        \n",
    "        self.progressbar = ttk.Progressbar(self.activeWidgetsFrame,orient =\"horizontal\",length = 200, mode =\"determinate\")\n",
    "        self.progressbar.grid(row=2, column=0, columnspan=2)\n",
    "        self.progressbar[\"maximum\"] = 100\n",
    "        self.progressbar[\"value\"] = 0\n",
    "        self.percentageLabel=tk.Label(self.activeWidgetsFrame,text=\"\",bg=self.secondBG)\n",
    "        self.percentageLabel.grid(row=3, column=0, columnspan=2,pady=10)\n",
    "        \n",
    "        self.progressLabel1=tk.Label(self.activeWidgetsFrame,text=\"\",bg=self.secondBG)\n",
    "        self.progressLabel1.grid(row=4, column=0, columnspan=2)\n",
    "        self.progressLabel2=tk.Label(self.activeWidgetsFrame,text=\"\",bg=self.secondBG)\n",
    "        self.progressLabel2.grid(row=5, column=0, columnspan=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start_GenerateTrainingSet_thread(Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_GenerateTrainingSet_thread(params):\n",
    "    th = threading.Thread(target=GenerateTrainingSet, kwargs=params)\n",
    "    th.start()\n",
    "#     th.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UpdateInterface(`int`, `int`, `object`, `str`, `str`)\n",
    "\n",
    "Updates dynamically the interface progress bar and labels through processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateInterface(maxValue, currentValue, app, pg1, pg2, percent):\n",
    "    app.progressLabel1['text'] = pg1\n",
    "    app.progressLabel2['text'] = pg2\n",
    "    app.progressbar[\"maximum\"]=maxValue\n",
    "    app.progressbar[\"value\"]=currentValue\n",
    "    app.percentageLabel['text'] = \"{}%\".format(percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Notebook\n",
    "\n",
    "The cell bellow starts the Notebook when the user is on a Jupyter environment. he will have to comment or uncomment the first or second part according to his wish to start the graphical interface or not.\n",
    "\n",
    "**Don't use this cell when executing the code in a .py script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix saved for date: 2015-01-05 00:00:00\n",
      "Sample 5 out of 5\n"
     ]
    }
   ],
   "source": [
    "# LAUNCH THE MAIN FUNCTION WITH GRAPHICS\n",
    "\n",
    "GenerateTrainingSet(indice='y2', \n",
    "                    startingDate=[2015,1,1], \n",
    "                    endingDate=[2015,1,6],\n",
    "                    area='asia',\n",
    "                    numberOfDaysPerMatrix=1, \n",
    "                    minutesBetweenValues=10, \n",
    "                    regressor='svr') # launch the main def\n",
    "\n",
    "\n",
    "# LAUNCH THE MAIN FUNCTION WITHIN THE GRAPHICAL INTERFACE CLASS\n",
    "# app_bg_color = '#180e0c'\n",
    "# app = Interface()\n",
    "# app.title(\"Dataset Generator\")\n",
    "# app.resizable(width=False, height=False)\n",
    "# appW=400\n",
    "# appH=350\n",
    "# posX = (int(app.winfo_screenwidth()) // 2) - (appW // 2)\n",
    "# posY = (int(app.winfo_screenheight()) // 2) - (appH // 2)\n",
    "# geo = \"{}x{}+{}+{}\".format(appW,appH,posX,posY)\n",
    "# app.geometry(geo)\n",
    "# app[\"bg\"]=app_bg_color\n",
    "# app.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Python Script\n",
    "\n",
    "This cell launches the main function in a .py script, checking if there is the `-g` (for \"graphics\") argument at the command's end.\n",
    "\n",
    "**Executing this cell in a Notebook won't have the expected behaviour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) > 1:\n",
    "    if sys.argv[1]=='-g':\n",
    "        app_bg_color = '#180e0c'\n",
    "        app = Interface()\n",
    "        app.title(\"Dataset Generator\")\n",
    "        app.resizable(width=False, height=False)\n",
    "        appW=400\n",
    "        appH=350\n",
    "        posX = (int(app.winfo_screenwidth()) // 2) - (appW // 2)\n",
    "        posY = (int(app.winfo_screenheight()) // 2) - (appH // 2)\n",
    "        geo = \"{}x{}+{}+{}\".format(appW,appH,posX,posY)\n",
    "        app.geometry(geo)\n",
    "        app[\"bg\"]=app_bg_color\n",
    "        app.mainloop()\n",
    "    else: print(\"Unknown parameter\")\n",
    "else:\n",
    "    GenerateTrainingSet(indice='y2', \n",
    "            startingDate=[2015,1,1], \n",
    "            endingDate=[2015,1,6],\n",
    "            area='asia',\n",
    "            numberOfDaysPerMatrix=1, \n",
    "            minutesBetweenValues=10, \n",
    "            regressor='svr') # launch the main def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
